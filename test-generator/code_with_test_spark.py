# Automatically generated by Pynguin.
from typing import List
import pyspark as ps


def func_0(dataset: List[int]):
    sc = ps.SparkContext()
    sc.setLogLevel('WARN')
    dataset_RDD =sc.parallelize(dataset)
    total = dataset_RDD.map(lambda i: i * 2).sum()
    print(total)
    return total



def test_case_0():
    int_0 = 2437
    list_0 = [int_0, int_0]
    var_0 =  func_0(list_0)
    assert var_0 == 9748


def test_case_1():
    int_0 = -2774
    list_0 = [int_0]
    var_0 =  func_0(list_0)
    assert var_0 == -2537
